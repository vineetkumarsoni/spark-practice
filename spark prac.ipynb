{"cells":[{"cell_type":"code","source":["\nprint('hello')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"927d9943-bc72-47d8-934e-38c5948b9f96","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["hello\n"]}],"execution_count":0},{"cell_type":"code","source":["d=[1,2,3,4,5,6]\nrdd=sc.parallelize(d,4)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ad47b115-ce0a-499f-8003-dfab7cad590c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e33899e0-8a40-428d-8edf-f5915fc08321","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[3]: [1, 2, 3, 4, 5, 6]"]}],"execution_count":0},{"cell_type":"code","source":["a=sc.parallelize([1,2,3,4,5,6,7,8,9])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff885f50-b09b-410a-9cbd-d1de0582da3f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["L=a.collect()\nprint(type(L))\nprint(L)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b4e2b34-b870-4e1e-b77e-071e7211932d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["<class 'list'>\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"]}],"execution_count":0},{"cell_type":"code","source":["a=sc.parallelize([1,2,3,4,5,6,7,8,9])\na.takeOrdered(6)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe324f70-1a2e-4007-9e3c-abd19d800121","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[6]: [1, 2, 3, 4, 5, 6]"]}],"execution_count":0},{"cell_type":"code","source":["a.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f9939c3-af0e-4050-86e4-2d5a4e70144a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[7]: 9"]}],"execution_count":0},{"cell_type":"code","source":["rdd.saveAsTextFile(\"/FileStore/rddvineet_2/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3049bee6-1e40-404b-9197-4488835b497f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-3267611801677658>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/FileStore/rddvineet_2/\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36msaveAsTextFile\u001B[0;34m(self, path, compressionCodecClass)\u001B[0m\n\u001B[1;32m   1860\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTextFileImpl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeyed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompressionCodecClass\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1861\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1862\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTextFileImpl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeyed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1863\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1864\u001B[0m     \u001B[0;31m# Pair functions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/rddvineet_2 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:75)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1601)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1601)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1587)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1587)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:560)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:559)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat org.apache.spark.api.python.PythonRDD$._saveAsTextFile(PythonRDD.scala:891)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsTextFileImpl(PythonRDD.scala:859)\n\tat org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/rddvineet_2 already exists","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n","\u001B[0;32m<command-3267611801677658>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n","\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/FileStore/rddvineet_2/\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\n","\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36msaveAsTextFile\u001B[0;34m(self, path, compressionCodecClass)\u001B[0m\n","\u001B[1;32m   1860\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTextFileImpl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeyed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompressionCodecClass\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1861\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m-> 1862\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTextFileImpl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeyed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m   1863\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1864\u001B[0m     \u001B[0;31m# Pair functions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n","\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n","\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n","\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n","\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n","\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n","\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl.\n",": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/rddvineet_2 already exists\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:75)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1601)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1601)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1587)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1587)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:560)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:559)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat org.apache.spark.api.python.PythonRDD$._saveAsTextFile(PythonRDD.scala:891)\n","\tat org.apache.spark.api.python.PythonRDD$.saveAsTextFileImpl(PythonRDD.scala:859)\n","\tat org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl(PythonRDD.scala)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n","\tat py4j.Gateway.invoke(Gateway.java:295)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n","\tat java.lang.Thread.run(Thread.java:748)\n"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize([(1,2),(2,4),(2,20),(3,4)])\nprint(\"Original RDD :\",rdd.collect())\nprint(\"After transformation :\",rdd.reduceByKey(lambda a,b:a+b).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3426a17e-14d6-4fe5-8def-b7842b3e0102","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize([(1,2),(2,4),(2,20),(3,4),(4,5),(7,8)])\nprint(\"Original RDD :\",rdd.collect())\nprint(\"After transformation :\",rdd.sortByKey().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e6e4374b-2d01-4607-8802-0f30cb9f802a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize([(1,2),(2,4),(2,20),(3,4),(4,5),(7,8)])\nprint(\"Original RDD :\",rdd.collect())\nprint(\"After transformation :\",rdd.mapValues(lambda x:x*x).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4383a45d-055a-44bc-8d5a-149c00ba5ef1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize([(1,2),(2,4),(2,20),(3,4),(4,5),(7,8)])\nprint(\"Original RDD :\",rdd.collect())\nprint(\"After transformation :\",rdd.groupByKey().mapValues(lambda x:[a+a for a in x]).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0eb89c9-65cd-407c-8707-fa18f699a1f8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nfrom pyspark.sql.types import Row,StructField,StructType, StringType,IntegerType\nsome_rdd =sc.parallelize([Row(name=\"john\",age=19),\n                         Row(name=\"smith\",age=23)])\nsome_rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0cbf7aa6-f3c9-453d-a4bb-1c8bbd6dc49f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[3]: [Row(name='john', age=19), Row(name='smith', age=23)]"]}],"execution_count":0},{"cell_type":"code","source":["some_df=spark.createDataFrame(some_rdd)\nsome_df.printSchema()\nsome_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73400f95-b891-4b1d-a7ab-19e831893ead","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n\n+-----+---+\n| name|age|\n+-----+---+\n| john| 19|\n|smith| 23|\n+-----+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[(\"vineet\",\"1997\"),(\"shrey\",\"2000\"),(\"ankit\",\"1994\")]\nrdd = spark.sparkContext.parallelize(data)\ndfFormRDD1 = spark.createDataFrame(rdd)\ndfFromRDD1.printSchema()\ndfFromRDD1.show()\n       \n       "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1506bde-88e9-4293-a05a-3bee93fa3e31","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1084939711449804>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mdfFormRDD1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdfFromRDD1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mdfFromRDD1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'dfFromRDD1' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'dfFromRDD1' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n","\u001B[0;32m<command-1084939711449804>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n","\u001B[1;32m      2\u001B[0m \u001B[0mrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m      3\u001B[0m \u001B[0mdfFormRDD1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdfFromRDD1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mdfFromRDD1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;31mNameError\u001B[0m: name 'dfFromRDD1' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["data = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\nrdd = spark.sparkContext.parallelize(data)\ndfFromRDD1 = spark.createDataFrame(rdd)\ndfFromRDD1.printSchema()\ndfFromRDD1.show()\n\ndfFromRDD2 = rdd.toDF([\"language\",\"users_count\"])\n#dfFromRDD2.printSchema()\ndfFromRDD2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5dbb243-8b07-45cf-be80-d71c46cf7b72","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+------+------+\n|    _1|    _2|\n+------+------+\n|  Java| 20000|\n|Python|100000|\n| Scala|  3000|\n+------+------+\n\n+--------+-----------+\n|language|users_count|\n+--------+-----------+\n|    Java|      20000|\n|  Python|     100000|\n|   Scala|       3000|\n+--------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import LongType\n# In this case we create the dataframe from an RDD of tuples (rather than Rows) and provide the schema explicitly\nanother_rdd = sc.parallelize([(\"John\", 19), (\"Smith\", 23), (\"Sarah\", 18)])\n# Schema with two fields - person_name and person_age\nschema = StructType([StructField(\"person_name\", LongType(), False),\n                     StructField(\"person_age\", IntegerType(), False)])\n\n# Create a DataFrame by applying the schema to the RDD and print the schema\nanother_df = sqlContext.createDataFrame(another_rdd, schema)\nanother_df.printSchema()\n# root\n#  |-- age: binteger (nullable = true)\n#  |-- name: string (nullable = true)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec6fb6b2-a34b-4a53-a59f-3c6d5efd4f06","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- person_name: long (nullable = false)\n |-- person_age: integer (nullable = false)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# when loading json files you can specify either a single file or a directory containing many json files.\npath = \"/FileStore/tables/new_1.json\"\n\n# Create a DataFrame from the file(s) pointed to by path\npeople_df = spark.read.json(path)\nprint('people is a',type(people_df))\n# The inferred schema can be visualized using the printSchema() method.\npeople_df.show()\n\npeople_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfa0cd22-0719-49e9-bd03-b58986b871bd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["people is a <class 'pyspark.sql.dataframe.DataFrame'>\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\nroot\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2=people_df.select(\"name\").where(people_df['name']=='Andy')\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a4abc12-6e9b-400a-b71d-80fbe66f8003","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+\n|name|\n+----+\n|Andy|\n+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2.write.parquet(\"/FileStore/tables/output5th\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"796f1652-9e51-4de6-80f1-20c31b5ffefc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh\n\nls -ltr /dbfs/FileStore/tables/output5th"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"511c2691-ded7-4134-8460-0b0b0097c974","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["ls: cannot access '/dbfs/FileStore/tables/output5th': No such file or directory\n"]}],"execution_count":0},{"cell_type":"code","source":["# import pyspark class Row from module sql\nfrom pyspark.sql import *\n\n# Create Example Data - Departments and Employees\n\n# Create the Departments\ndepartment1 = Row(id='123456', name='Computer Science')\ndepartment2 = Row(id='789012', name='Mechanical Engineering')\ndepartment3 = Row(id='345678', name='Theater and Drama')\ndepartment4 = Row(id='901234', name='Indoor Recreation')\n\n# Create the Employees\nEmployee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\nemployee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\nemployee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\nemployee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\nemployee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\nemployee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000)\n\n# Create the DepartmentWithEmployees instances from Departments and Employees\ndepartmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\ndepartmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\ndepartmentWithEmployees3 = Row(department=department3, employees=[employee5, employee4])\ndepartmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2ad98ae-f3c3-4e70-832d-42f11a8dcf71","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\ndf1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n\ndisplay(df1)\n\ndepartmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\ndf2 = spark.createDataFrame(departmentsWithEmployeesSeq2)\n\ndisplay(df2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"17e03214-cb3c-4d11-9edd-f53de83286de","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[["123456","Computer Science"],[["michael","armbrust","no-reply@berkeley.edu",100000],["xiangrui","meng","no-reply@stanford.edu",120000]]],[["789012","Mechanical Engineering"],[["matei",null,"no-reply@waterloo.edu",140000],[null,"wendell","no-reply@berkeley.edu",160000]]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"department","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"employees","type":"{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"firstName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lastName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"email\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>employees</th></tr></thead><tbody><tr><td>List(123456, Computer Science)</td><td>List(List(michael, armbrust, no-reply@berkeley.edu, 100000), List(xiangrui, meng, no-reply@stanford.edu, 120000))</td></tr><tr><td>List(789012, Mechanical Engineering)</td><td>List(List(matei, null, no-reply@waterloo.edu, 140000), List(null, wendell, no-reply@berkeley.edu, 160000))</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[["345678","Theater and Drama"],[["michael","jackson","no-reply@neverla.nd",80000],[null,"wendell","no-reply@berkeley.edu",160000]]],[["901234","Indoor Recreation"],[["xiangrui","meng","no-reply@stanford.edu",120000],["matei",null,"no-reply@waterloo.edu",140000]]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"department","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"employees","type":"{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"firstName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lastName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"email\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>employees</th></tr></thead><tbody><tr><td>List(345678, Theater and Drama)</td><td>List(List(michael, jackson, no-reply@neverla.nd, 80000), List(null, wendell, no-reply@berkeley.edu, 160000))</td></tr><tr><td>List(901234, Indoor Recreation)</td><td>List(List(xiangrui, meng, no-reply@stanford.edu, 120000), List(matei, null, no-reply@waterloo.edu, 140000))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df2.select(\"department.name\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40682f8b-dfcf-4a06-b3fa-8ef5474e15af","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+\n|             name|\n+-----------------+\n|Theater and Drama|\n|Indoor Recreation|\n+-----------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2.select(\"employees.email\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6648ca56-bbfe-4b61-9613-4bedb9d22662","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+\n|               email|\n+--------------------+\n|[no-reply@neverla...|\n|[no-reply@stanfor...|\n+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["unionDF = df1.union(df2)\ndisplay(unionDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2ae47a3-e2b5-4157-a7b2-010dc6e15394","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[["123456","Computer Science"],[["michael","armbrust","no-reply@berkeley.edu",100000],["xiangrui","meng","no-reply@stanford.edu",120000]]],[["789012","Mechanical Engineering"],[["matei",null,"no-reply@waterloo.edu",140000],[null,"wendell","no-reply@berkeley.edu",160000]]],[["345678","Theater and Drama"],[["michael","jackson","no-reply@neverla.nd",80000],[null,"wendell","no-reply@berkeley.edu",160000]]],[["901234","Indoor Recreation"],[["xiangrui","meng","no-reply@stanford.edu",120000],["matei",null,"no-reply@waterloo.edu",140000]]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"department","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"employees","type":"{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"firstName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lastName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"email\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>employees</th></tr></thead><tbody><tr><td>List(123456, Computer Science)</td><td>List(List(michael, armbrust, no-reply@berkeley.edu, 100000), List(xiangrui, meng, no-reply@stanford.edu, 120000))</td></tr><tr><td>List(789012, Mechanical Engineering)</td><td>List(List(matei, null, no-reply@waterloo.edu, 140000), List(null, wendell, no-reply@berkeley.edu, 160000))</td></tr><tr><td>List(345678, Theater and Drama)</td><td>List(List(michael, jackson, no-reply@neverla.nd, 80000), List(null, wendell, no-reply@berkeley.edu, 160000))</td></tr><tr><td>List(901234, Indoor Recreation)</td><td>List(List(xiangrui, meng, no-reply@stanford.edu, 120000), List(matei, null, no-reply@waterloo.edu, 140000))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["unionDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19d48ba6-7262-4dd4-9ca6-fc9a73d289e9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- department: struct (nullable = true)\n |    |-- id: string (nullable = true)\n |    |-- name: string (nullable = true)\n |-- employees: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- firstName: string (nullable = true)\n |    |    |-- lastName: string (nullable = true)\n |    |    |-- email: string (nullable = true)\n |    |    |-- salary: long (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["unionDF.select(\"employees.firstName\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81166b8f-79ec-49ce-9b2d-d8179011b1e1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------+\n|          firstName|\n+-------------------+\n|[michael, xiangrui]|\n|      [matei, null]|\n|    [michael, null]|\n|  [xiangrui, matei]|\n+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n#exlode  se multiple column ko row me change krta hai\nexplodeDF = unionDF.select(explode(\"employees.firstName\"))\nexplodeDF.show(4,truncate= False)\nunexplodeDF = unionDF.select(\"employees.firstName\")\nunexplodeDF.show(2,truncate= False)\nunionDF.printSchema()\nexplodeDF.printSchema()\nunexplodeDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78b8de29-5ce7-40ff-b8de-c662861fdbf9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------+\n|col     |\n+--------+\n|michael |\n|xiangrui|\n|matei   |\n|null    |\n+--------+\nonly showing top 4 rows\n\n+-------------------+\n|firstName          |\n+-------------------+\n|[michael, xiangrui]|\n|[matei, null]      |\n+-------------------+\nonly showing top 2 rows\n\nroot\n |-- department: struct (nullable = true)\n |    |-- id: string (nullable = true)\n |    |-- name: string (nullable = true)\n |-- employees: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- firstName: string (nullable = true)\n |    |    |-- lastName: string (nullable = true)\n |    |    |-- email: string (nullable = true)\n |    |    |-- salary: long (nullable = true)\n\nroot\n |-- col: string (nullable = true)\n\nroot\n |-- firstName: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["explodeDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9186699e-d8b4-4180-920b-93b6cf0a3e72","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- col: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nfrom pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n# One way to create a DataFrame is to first define an RDD from a list of Rows \nsome_rdd = sc.parallelize([Row(name=\"John\", age=19),\n                           Row(name=\"Smith\", age=23),\n                           Row(name=\"Sarah\", age=18)])\nsome_rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a92bf8b5-28c4-4ca7-aa02-f8c3a2df0f3a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[50]: [Row(name='John', age=19),\n Row(name='Smith', age=23),\n Row(name='Sarah', age=18)]"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions.flatten\n\nfilterDF= flattenDF.filter(flattenDF.firstName == \"xiangrui\").sort(flattenDF.lastName)\ndisplay(filterDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4200f7c-9598-4f83-9714-11b9d3c5a81e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-4219031342900709>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    from pyspark.sql.functions.flatten\u001B[0m\n\u001B[0m                                      ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-4219031342900709>, line 1)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-4219031342900709>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n","\u001B[0;31m    from pyspark.sql.functions.flatten\u001B[0m\n","\u001B[0m                                      ^\u001B[0m\n","\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col, asc\nfilterDF = flattenDF.select(\"email\").filter(flattenDF.firstName == \"xiangrui\").sort(flattenDF.lastName)\ndisplay(filterDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20ff4f0d-4b1d-4fcd-b5a5-823a1ae98d86","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3890275567092957>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0masc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mfilterDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mflattenDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"email\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflattenDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfirstName\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"xiangrui\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msort\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflattenDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlastName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilterDF\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'flattenDF' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'flattenDF' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n","\u001B[0;32m<command-3890275567092957>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n","\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0masc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m----> 2\u001B[0;31m \u001B[0mfilterDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mflattenDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"email\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflattenDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfirstName\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"xiangrui\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msort\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflattenDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlastName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilterDF\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;31mNameError\u001B[0m: name 'flattenDF' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\n\ncountDistinctDF = whereDF.select(\"firstName\", \"lastName\")\\\n  .groupBy(\"firstName\")\\\n  .agg(countDistinct(\"lastName\").alias(\"distinct_last_names\"))\n\ndisplay(countDistinctDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a0fad9b-8634-4631-955c-58f4b541d724","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3890275567092958>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcountDistinct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mcountDistinctDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwhereDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"firstName\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"lastName\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0mgroupBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"firstName\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0magg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcountDistinct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"lastName\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"distinct_last_names\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'whereDF' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'whereDF' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n","\u001B[0;32m<command-3890275567092958>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n","\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcountDistinct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m----> 3\u001B[0;31m \u001B[0mcountDistinctDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwhereDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"firstName\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"lastName\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m      4\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0mgroupBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"firstName\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m      5\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0magg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcountDistinct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"lastName\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"distinct_last_names\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;31mNameError\u001B[0m: name 'whereDF' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3ace987-98ff-4c70-8dfd-6eca0ccfd173","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"spark prac","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3890275567092952,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
